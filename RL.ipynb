{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b184d315",
   "metadata": {},
   "source": [
    "# Connect Four â€“ Maskable PPO (Colab/Kaggle ready)\n",
    "\n",
    "This notebook trains SB3-contrib `MaskablePPO` on the custom Connect Four env, logs evals vs fixed opponents, runs an ablation, and plots win rates/loss curves.\n",
    "\n",
    "Steps:\n",
    "1. Install deps.\n",
    "2. Clone/pull repo and `cd` into `rl_connect4`.\n",
    "3. Run smoke training, ablation, and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (rerun on fresh runtime)\n",
    "!pip install -q stable-baselines3[extra] sb3-contrib gymnasium pygame numpy torch pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868cf791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "\n",
    "REPO_URL = \"https://github.com/UmerSR/Connect-Four-RL.git\"\n",
    "WORKSPACE = \"/content/Connect-Four-RL\"\n",
    "\n",
    "if not os.path.exists(WORKSPACE):\n",
    "    !git clone $REPO_URL $WORKSPACE\n",
    "else:\n",
    "    %cd $WORKSPACE\n",
    "    !git pull --ff-only\n",
    "\n",
    "%cd $WORKSPACE/rl_connect4\n",
    "sys.path.insert(0, WORKSPACE)\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from envs.connect_four_env import ConnectFourEnv\n",
    "from agents.simple_agents import RandomAgent, HeuristicAgent\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694808eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env builders with action masking\n",
    "def mask_fn(arg):\n",
    "    if isinstance(arg, dict):\n",
    "        return arg[\"action_mask\"].astype(bool)\n",
    "    if hasattr(arg, \"action_masks\"):\n",
    "        return arg.action_masks()\n",
    "    raise TypeError(f\"Unsupported mask_fn input: {type(arg)}\")\n",
    "\n",
    "def make_env(seed=None):\n",
    "    def _init():\n",
    "        env = ConnectFourEnv(seed=seed)\n",
    "        env = ActionMasker(env, mask_fn)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_vec_envs(n_envs=4, seed=0):\n",
    "    env_fns = [make_env(seed + i if seed is not None else None) for i in range(n_envs)]\n",
    "    vec = DummyVecEnv(env_fns)\n",
    "    vec = VecMonitor(vec)\n",
    "    return vec\n",
    "\n",
    "eval_env = ActionMasker(ConnectFourEnv(), mask_fn)\n",
    "obs, _ = eval_env.reset()\n",
    "print(\"Obs keys:\", obs.keys(), \"action_mask shape:\", obs[\"action_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9774925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced evaluation helpers\n",
    "def evaluate_vs_opponent(model, env, opponent, episodes=20, deterministic=False):\n",
    "    results = {\"wins\": 0, \"losses\": 0, \"draws\": 0, \"illegal\": 0}\n",
    "    for _ in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        info = {}\n",
    "        while not done:\n",
    "            if env.unwrapped.current_player == 0:\n",
    "                action, _ = model.predict(obs, deterministic=deterministic)\n",
    "            else:\n",
    "                action = opponent.select_action(env.unwrapped)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        if info.get(\"illegal_move\"):\n",
    "            results[\"illegal\"] += 1\n",
    "        elif info.get(\"draw\"):\n",
    "            results[\"draws\"] += 1\n",
    "        elif info.get(\"winner\") == 0:\n",
    "            results[\"wins\"] += 1\n",
    "        else:\n",
    "            results[\"losses\"] += 1\n",
    "    results[\"win_rate\"] = results[\"wins\"] / max(1, episodes)\n",
    "    return results\n",
    "\n",
    "class EnhancedEvalRecorder(BaseCallback):\n",
    "    def __init__(self, eval_env, eval_freq=2000, n_eval_episodes=50):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.history = []\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            mean_r, std_r = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                warn=False,\n",
    "                deterministic=False,\n",
    "            )\n",
    "            rand_eval = evaluate_vs_opponent(\n",
    "                self.model,\n",
    "                ActionMasker(ConnectFourEnv(), mask_fn),\n",
    "                RandomAgent(),\n",
    "                episodes=20,\n",
    "                deterministic=False,\n",
    "            )\n",
    "            heur_eval = evaluate_vs_opponent(\n",
    "                self.model,\n",
    "                ActionMasker(ConnectFourEnv(), mask_fn),\n",
    "                HeuristicAgent(),\n",
    "                episodes=20,\n",
    "                deterministic=False,\n",
    "            )\n",
    "            self.history.append({\n",
    "                \"timesteps\": self.num_timesteps,\n",
    "                \"mean_reward\": mean_r,\n",
    "                \"std_reward\": std_r,\n",
    "                \"win_rate_random\": rand_eval[\"win_rate\"],\n",
    "                \"win_rate_heuristic\": heur_eval[\"win_rate\"],\n",
    "            })\n",
    "            self.best_mean_reward = max(self.best_mean_reward, mean_r)\n",
    "        return True\n",
    "\n",
    "def train_maskable_ppo(run_name: str, total_timesteps: int, hyperparams: dict, n_envs: int = 4, eval_freq: int = 2000, n_eval_episodes: int = 50):\n",
    "    log_dir = f\"/content/Connect-Four-RL/outputs/{run_name}\"\n",
    "    pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    vec_env = make_vec_envs(n_envs=n_envs)\n",
    "    model = MaskablePPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=device,\n",
    "        **hyperparams,\n",
    "    )\n",
    "    model.set_logger(configure(log_dir, [\"stdout\", \"csv\"]))\n",
    "\n",
    "    callback = EnhancedEvalRecorder(eval_env, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=False)\n",
    "\n",
    "    mean_r, std_r = evaluate_policy(\n",
    "        model,\n",
    "        eval_env,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        warn=False,\n",
    "        deterministic=False,\n",
    "    )\n",
    "    rand_eval = evaluate_vs_opponent(model, ActionMasker(ConnectFourEnv(), mask_fn), RandomAgent(), episodes=50, deterministic=False)\n",
    "    heur_eval = evaluate_vs_opponent(model, ActionMasker(ConnectFourEnv(), mask_fn), HeuristicAgent(), episodes=50, deterministic=False)\n",
    "    callback.history.append({\n",
    "        \"timesteps\": total_timesteps,\n",
    "        \"mean_reward\": mean_r,\n",
    "        \"std_reward\": std_r,\n",
    "        \"win_rate_random\": rand_eval[\"win_rate\"],\n",
    "        \"win_rate_heuristic\": heur_eval[\"win_rate\"],\n",
    "    })\n",
    "\n",
    "    return model, callback.history, log_dir\n",
    "\n",
    "hyperparams_example = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 1024,\n",
    "    \"gae_lambda\": 0.95,\n",
    "}\n",
    "hyperparams_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617543e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke run\n",
    "smoke_timesteps = 20000\n",
    "smoke_model, smoke_history, smoke_logdir = train_maskable_ppo(\n",
    "    run_name=\"smoke\",\n",
    "    total_timesteps=smoke_timesteps,\n",
    "    hyperparams=hyperparams_example,\n",
    "    n_envs=4,\n",
    "    eval_freq=2000,\n",
    "    n_eval_episodes=50,\n",
    ")\n",
    "pd.DataFrame(smoke_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80dd000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation over common hyperparameters\n",
    "ablation_grid = [\n",
    "    {\"tag\": \"lr_3e-4_clip_0.2\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_1e-3_clip_0.2\", \"learning_rate\": 1e-3, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_clip_0.1\", \"learning_rate\": 3e-4, \"clip_range\": 0.1, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_gamma_0.995\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.995, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_ent_0.02\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95, \"ent_coef\": 0.02},\n",
    "]\n",
    "\n",
    "common_params = {\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 1024,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"ent_coef\": 0.01,\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "all_histories = []\n",
    "\n",
    "total_timesteps = 200000\n",
    "for cfg in ablation_grid:\n",
    "    hyper_cfg = {k: v for k, v in cfg.items() if k != \"tag\"}\n",
    "    hyper = {**common_params, **hyper_cfg}\n",
    "    run_name = f\"ablate_{cfg['tag']}\"\n",
    "    print(f\"\\n=== Training {run_name} ===\")\n",
    "    model, history, log_dir = train_maskable_ppo(\n",
    "        run_name=run_name,\n",
    "        total_timesteps=total_timesteps,\n",
    "        hyperparams=hyper,\n",
    "        n_envs=8,\n",
    "        eval_freq=2000,\n",
    "        n_eval_episodes=50,\n",
    "    )\n",
    "    if len(history) > 0:\n",
    "        best = max(history, key=lambda h: h.get(\"win_rate_random\", h.get(\"mean_reward\", -np.inf)))\n",
    "        best_mean = best.get(\"mean_reward\", np.nan)\n",
    "        best_wr_rand = best.get(\"win_rate_random\", np.nan)\n",
    "        best_wr_heur = best.get(\"win_rate_heuristic\", np.nan)\n",
    "    else:\n",
    "        best_mean = np.nan\n",
    "        best_wr_rand = np.nan\n",
    "        best_wr_heur = np.nan\n",
    "\n",
    "    ablation_results.append({\n",
    "        \"run\": run_name,\n",
    "        \"best_mean_reward\": best_mean,\n",
    "        \"best_win_rate_random\": best_wr_rand,\n",
    "        \"best_win_rate_heuristic\": best_wr_heur,\n",
    "        \"log_dir\": log_dir,\n",
    "        **cfg,\n",
    "    })\n",
    "    hist_df = pd.DataFrame(history)\n",
    "    hist_df[\"run\"] = run_name\n",
    "    all_histories.append(hist_df)\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "ablation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21957491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eval curves (mean reward and win rates)\n",
    "if all_histories:\n",
    "    hist_all_df = pd.concat(all_histories, ignore_index=True)\n",
    "    if hist_all_df.empty:\n",
    "        print(\"No eval data recorded yet.\")\n",
    "    else:\n",
    "        if \"timesteps\" not in hist_all_df.columns and \"time/total_timesteps\" in hist_all_df.columns:\n",
    "            hist_all_df = hist_all_df.rename(columns={\"time/total_timesteps\": \"timesteps\"})\n",
    "\n",
    "        if \"timesteps\" in hist_all_df.columns and \"mean_reward\" in hist_all_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(data=hist_all_df, x=\"timesteps\", y=\"mean_reward\", hue=\"run\", marker=\"o\")\n",
    "            plt.title(\"Evaluation mean reward vs timesteps\")\n",
    "            plt.show()\n",
    "        if \"timesteps\" in hist_all_df.columns and \"win_rate_random\" in hist_all_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(data=hist_all_df, x=\"timesteps\", y=\"win_rate_random\", hue=\"run\", marker=\"o\")\n",
    "            plt.title(\"Win rate vs Random vs timesteps\")\n",
    "            plt.show()\n",
    "        if \"timesteps\" in hist_all_df.columns and \"win_rate_heuristic\" in hist_all_df.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.lineplot(data=hist_all_df, x=\"timesteps\", y=\"win_rate_heuristic\", hue=\"run\", marker=\"o\")\n",
    "            plt.title(\"Win rate vs Heuristic vs timesteps\")\n",
    "            plt.show()\n",
    "\n",
    "    if not ablation_df.empty:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.barplot(data=ablation_df, x=\"run\", y=\"best_win_rate_random\")\n",
    "        plt.xticks(rotation=30, ha=\"right\")\n",
    "        plt.title(\"Best win rate vs Random per run\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No ablation summary to plot yet.\")\n",
    "else:\n",
    "    print(\"No history to plot yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses from the first run's CSV logs (if present)\n",
    "loss_keys = [\n",
    "    \"train/policy_gradient_loss\",\n",
    "    \"train/value_loss\",\n",
    "    \"train/entropy_loss\",\n",
    "    \"train/approx_kl\",\n",
    "]\n",
    "\n",
    "if len(ablation_results) > 0:\n",
    "    sample_log = pathlib.Path(ablation_results[0][\"log_dir\"]) / \"progress.csv\"\n",
    "elif 'smoke_logdir' in globals():\n",
    "    sample_log = pathlib.Path(smoke_logdir) / \"progress.csv\"\n",
    "else:\n",
    "    sample_log = None\n",
    "\n",
    "if sample_log and sample_log.exists():\n",
    "    df_log = pd.read_csv(sample_log)\n",
    "    available = [k for k in loss_keys if k in df_log.columns]\n",
    "    if available:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for k in available:\n",
    "            plt.plot(df_log[\"time/total_timesteps\"], df_log[k], label=k)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.title(\"Training losses / diagnostics\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No loss keys available in progress.csv\")\n",
    "else:\n",
    "    print(\"No progress.csv found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
