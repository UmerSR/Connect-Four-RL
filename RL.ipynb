{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91272754",
   "metadata": {},
   "source": [
    "# Connect Four â€“ Maskable PPO (Colab-ready)\n",
    "\n",
    "Run this notebook on Colab/Kaggle (GPU optional). It trains out-of-the-box PPO using `sb3-contrib`'s `MaskablePPO`, collects evaluation metrics, and plots ablation results and training losses.\n",
    "\n",
    "**Steps:**\n",
    "1. Install deps (first cell).\n",
    "2. Clone/pull repo and `cd` into `rl_connect4`.\n",
    "3. Run the training/ablation cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a262e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (rerun on a fresh Colab runtime)\n",
    "!pip install -q stable-baselines3[extra] sb3-contrib gymnasium pygame numpy torch pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba80ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, pathlib\n",
    "\n",
    "REPO_URL = \"https://github.com/UmerSR/Connect-Four-RL.git\"\n",
    "WORKSPACE = \"/content/Connect-Four-RL\"\n",
    "\n",
    "if not os.path.exists(WORKSPACE):\n",
    "    !git clone $REPO_URL $WORKSPACE\n",
    "else:\n",
    "    %cd $WORKSPACE\n",
    "    !git pull --ff-only\n",
    "\n",
    "%cd $WORKSPACE/rl_connect4\n",
    "sys.path.insert(0, WORKSPACE)  # allow imports like envs.connect_four_env\n",
    "print(\"CWD:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from envs.connect_four_env import ConnectFourEnv\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ee180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env builders with action masking\n",
    "def mask_fn(obs):\n",
    "    # obs is a dict with 'action_mask'\n",
    "    return obs[\"action_mask\"].astype(bool)\n",
    "\n",
    "def make_env(seed=None):\n",
    "    def _init():\n",
    "        env = ConnectFourEnv(seed=seed)\n",
    "        env = ActionMasker(env, mask_fn)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_vec_envs(n_envs=4, seed=0):\n",
    "    env_fns = [make_env(seed + i if seed is not None else None) for i in range(n_envs)]\n",
    "    vec = DummyVecEnv(env_fns)\n",
    "    vec = VecMonitor(vec)\n",
    "    return vec\n",
    "\n",
    "eval_env = ActionMasker(ConnectFourEnv(), mask_fn)\n",
    "obs, _ = eval_env.reset()\n",
    "print(\"Obs keys:\", obs.keys(), \"action_mask shape:\", obs[\"action_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90679bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to record periodic evaluation\n",
    "class EvalRecorder(BaseCallback):\n",
    "    def __init__(self, eval_env, eval_freq=5000, n_eval_episodes=20):\n",
    "        super().__init__()\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.history = []\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            mean_r, std_r = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                warn=False,\n",
    "                deterministic=True,\n",
    "            )\n",
    "            self.history.append({\n",
    "                \"timesteps\": self.num_timesteps,\n",
    "                \"mean_reward\": mean_r,\n",
    "                \"std_reward\": std_r,\n",
    "            })\n",
    "            self.best_mean_reward = max(self.best_mean_reward, mean_r)\n",
    "        return True\n",
    "\n",
    "def train_maskable_ppo(run_name: str, total_timesteps: int, hyperparams: dict, n_envs: int = 4, eval_freq: int = 5000, n_eval_episodes: int = 20):\n",
    "    log_dir = f\"/content/Connect-Four-RL/outputs/{run_name}\"\n",
    "    pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    vec_env = make_vec_envs(n_envs=n_envs)\n",
    "    model = MaskablePPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        vec_env,\n",
    "        verbose=0,\n",
    "        tensorboard_log=log_dir,\n",
    "        device=device,\n",
    "        **hyperparams,\n",
    "    )\n",
    "    model.set_logger(configure(log_dir, [\"stdout\", \"csv\"]))\n",
    "\n",
    "    callback = EvalRecorder(eval_env, eval_freq=eval_freq, n_eval_episodes=n_eval_episodes)\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callback, progress_bar=True)\n",
    "    return model, callback.history, log_dir\n",
    "\n",
    "hyperparams_example = {\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"clip_range\": 0.2,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 1024,\n",
    "    \"gae_lambda\": 0.95,\n",
    "}\n",
    "hyperparams_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke run (adjust timesteps down for faster iteration)\n",
    "smoke_timesteps = 20000\n",
    "smoke_model, smoke_history, smoke_logdir = train_maskable_ppo(\n",
    "    run_name=\"smoke\",\n",
    "    total_timesteps=smoke_timesteps,\n",
    "    hyperparams=hyperparams_example,\n",
    "    n_envs=4,\n",
    "    eval_freq=5000,\n",
    "    n_eval_episodes=10,\n",
    ")\n",
    "pd.DataFrame(smoke_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b84273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation over common hyperparameters\n",
    "ablation_grid = [\n",
    "    {\"tag\": \"lr_3e-4_clip_0.2\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_1e-3_clip_0.2\", \"learning_rate\": 1e-3, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_clip_0.1\", \"learning_rate\": 3e-4, \"clip_range\": 0.1, \"gamma\": 0.99, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_gamma_0.995\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.995, \"gae_lambda\": 0.95},\n",
    "    {\"tag\": \"lr_3e-4_ent_0.02\", \"learning_rate\": 3e-4, \"clip_range\": 0.2, \"gamma\": 0.99, \"gae_lambda\": 0.95, \"ent_coef\": 0.02},\n",
    "]\n",
    "\n",
    "# Shared params\n",
    "common_params = {\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 1024,\n",
    "    \"vf_coef\": 0.5,\n",
    "    \"ent_coef\": 0.01,\n",
    "}\n",
    "\n",
    "ablation_results = []\n",
    "all_histories = []\n",
    "\n",
    "total_timesteps = 50000  # adjust up for deeper training\n",
    "for cfg in ablation_grid:\n",
    "    hyper = {**common_params, **cfg}\n",
    "    run_name = f\"ablate_{cfg['tag']}\"\n",
    "    print(f\"\\n=== Training {run_name} ===\")\n",
    "    model, history, log_dir = train_maskable_ppo(\n",
    "        run_name=run_name,\n",
    "        total_timesteps=total_timesteps,\n",
    "        hyperparams=hyper,\n",
    "        n_envs=8,\n",
    "        eval_freq=10000,\n",
    "        n_eval_episodes=15,\n",
    "    )\n",
    "    if len(history) > 0:\n",
    "        best = max(history, key=lambda h: h[\"mean_reward\"])\n",
    "        best_mean = best[\"mean_reward\"]\n",
    "    else:\n",
    "        best_mean = np.nan\n",
    "\n",
    "    ablation_results.append({\n",
    "        \"run\": run_name,\n",
    "        \"best_mean_reward\": best_mean,\n",
    "        \"log_dir\": log_dir,\n",
    "        **cfg,\n",
    "    })\n",
    "    hist_df = pd.DataFrame(history)\n",
    "    hist_df[\"run\"] = run_name\n",
    "    all_histories.append(hist_df)\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "ablation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot eval curves\n",
    "if all_histories:\n",
    "    hist_all_df = pd.concat(all_histories)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(data=hist_all_df, x=\"timesteps\", y=\"mean_reward\", hue=\"run\", marker=\"o\")\n",
    "    plt.title(\"Evaluation mean reward vs timesteps\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(data=ablation_df, x=\"run\", y=\"best_mean_reward\")\n",
    "    plt.xticks(rotation=30, ha=\"right\")\n",
    "    plt.title(\"Best mean reward per run\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No history to plot yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033d7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses from the first run's CSV logs (if present)\n",
    "loss_keys = [\n",
    "    \"train/policy_gradient_loss\",\n",
    "    \"train/value_loss\",\n",
    "    \"train/entropy_loss\",\n",
    "    \"train/approx_kl\",\n",
    "]\n",
    "\n",
    "if len(ablation_results) > 0:\n",
    "    sample_log = pathlib.Path(ablation_results[0][\"log_dir\"]) / \"progress.csv\"\n",
    "elif 'smoke_logdir' in globals():\n",
    "    sample_log = pathlib.Path(smoke_logdir) / \"progress.csv\"\n",
    "else:\n",
    "    sample_log = None\n",
    "\n",
    "if sample_log and sample_log.exists():\n",
    "    df_log = pd.read_csv(sample_log)\n",
    "    available = [k for k in loss_keys if k in df_log.columns]\n",
    "    if available:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for k in available:\n",
    "            plt.plot(df_log[\"time/total_timesteps\"], df_log[k], label=k)\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Timesteps\")\n",
    "        plt.title(\"Training losses / diagnostics\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No loss keys available in progress.csv\")\n",
    "else:\n",
    "    print(\"No progress.csv found yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
